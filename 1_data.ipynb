{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClimateHack.AI 2023: Data Exploration\n",
    "\n",
    "Thank you for participating in ClimateHack.AI 2023! üåç\n",
    "\n",
    "Your contributions could help cut carbon emissions by up to 100 kilotonnes per year in Great Britain alone. We look forward to seeing what you build over the course of the competition!\n",
    "\n",
    "As with any machine learning task, the best place to start is by inspecting the data available, and for this competition, we are spoiled for choice! In total, 600 GB of (mostly compressed) training data is available for you to use and experiment with, including 73 GB of [HRV satellite imagery](https://huggingface.co/datasets/climatehackai/climatehackai-2023/tree/main/satellite-hrv), 189 GB of [non-HRV satellite imagery](https://huggingface.co/datasets/climatehackai/climatehackai-2023/tree/main/satellite-nonhrv), 78 GB of [weather forecasts](https://huggingface.co/datasets/climatehackai/climatehackai-2023/tree/main/weather), 259 GB of [air quality forecasts](https://huggingface.co/datasets/climatehackai/climatehackai-2023/tree/main/aerosols) and ~1 GB of [historical solar PV generation data](https://huggingface.co/datasets/climatehackai/climatehackai-2023/tree/main/pv).\n",
    "\n",
    "You do not have you use all of the data for this challenge (and in fact, you probably shouldn't lest you end up with a bloated model!). Having said that, it is up to you to **be creative** to decide which data sources you actually do want to use and train on! Ultimately, you want to build an **accurate and performant machine learning model** capable of extracting as much useful information from the available data as possible in order to maximally explain any variance in solar PV yields.\n",
    "\n",
    "For more detailed information on the available data, see the [DOXA AI competition page](https://doxaai.com/competition/climatehackai-2023/overview). üòé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "If you do not have the following Python packages installed, you can uncomment and run the following line to install them with `pip`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy matplotlib zarr xarray ipykernel gcsfs fsspec dask cartopy ocf-blosc2\n",
    "# %pip install -U doxa-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from ocf_blosc2 import Blosc2\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Zarr datasets\n",
    "\n",
    "Most of the data for this competition uses the [Zarr storage format](https://zarr.dev/), which allows for incredibly large NumPy-like multi-dimensional arrays to be split and stored as individually compressed chunks in a relatively efficient way while still letting you interact with the data as if it were one contiguous whole. This means that you do not in theory have to load all of the data you wish to use into memory at once.\n",
    "\n",
    "One benefit of the Zarr format is that Zarr datasets can be streamed straight from the cloud. While this most likely will not be fast enough in training, it already lets us perform some initial data exploration without having to download entire months of data.\n",
    "\n",
    "In Python, we can work with Zarr stores using [xarray](https://docs.xarray.dev/en/stable/user-guide/io.html#zarr), as we will see!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HRV Satellite Imagery\n",
    "\n",
    "We can load the high-resolution visible (HRV) EUMETSAT satellite imagery data straight from [HuggingFace](https://huggingface.co/datasets/climatehackai/climatehackai-2023/tree/main/satellite-hrv). The satellite imagery covers an area slightly larger than Great Britain and is taken every five minutes. Since this data has a much higher resolution than the non-HRV satellite imagery data, clouds are a lot more visible, which may be of particular interest to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrv = xr.open_dataset(\n",
    "    \"zip:///::https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2020/7.zarr.zip\",\n",
    "    engine=\"zarr\",\n",
    "    consolidated=True,\n",
    ")\n",
    "\n",
    "hrv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.plot()` method to take a look at what the HRV data looks like at a particular moment in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrv[\"data\"].sel(time=\"2020-07-20 10:00\").plot()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly more advanced version of this allows us to draw coastlines on top of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes(projection=ccrs.Geostationary(central_longitude=9.5))\n",
    "\n",
    "hrv[\"data\"].sel(time=\"2020-07-20 10:00\", channel=\"HRV\").plot.pcolormesh(\n",
    "    ax=axes,\n",
    "    transform=ccrs.Geostationary(central_longitude=9.5),\n",
    "    x=\"x_geostationary\",\n",
    "    y=\"y_geostationary\",\n",
    "    add_colorbar=False,\n",
    ")  # type: ignore\n",
    "\n",
    "axes.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this challenge, you are given 128x128 satellite imagery crops centred on each solar PV site taken over the previous hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 12, figsize=(15, 3))\n",
    "\n",
    "for j, time in enumerate(\n",
    "    [\n",
    "        \"2020-07-04 12:00\",\n",
    "        \"2020-07-04 12:05\",\n",
    "        \"2020-07-04 12:10\",\n",
    "        \"2020-07-04 12:15\",\n",
    "        \"2020-07-04 12:20\",\n",
    "        \"2020-07-04 12:25\",\n",
    "        \"2020-07-04 12:30\",\n",
    "        \"2020-07-04 12:35\",\n",
    "        \"2020-07-04 12:40\",\n",
    "        \"2020-07-04 12:45\",\n",
    "        \"2020-07-04 12:50\",\n",
    "        \"2020-07-04 12:55\",\n",
    "    ]\n",
    "):\n",
    "    ax[j].imshow(\n",
    "        hrv[\"data\"]\n",
    "        .sel(time=time)\n",
    "        .isel(x_geostationary=slice(128, 256), y_geostationary=slice(128, 256))\n",
    "        .to_numpy(),\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "    ax[j].get_xaxis().set_visible(False)\n",
    "    ax[j].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-HRV Satellite Imagery\n",
    "\n",
    "We can also perform something similar for the non-HRV satellite imagery data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonhrv = xr.open_dataset(\n",
    "    \"zip:///::https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-nonhrv/2020/7.zarr.zip\",\n",
    "    engine=\"zarr\",\n",
    "    consolidated=True,\n",
    ")\n",
    "\n",
    "nonhrv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the non-HRV satellite imagery data is composed of 11 different channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonhrv.channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 11 non-HRV channels correspond to different wavelengths and types of satellite imagery (visible, infrared and water vapour).\n",
    "\n",
    "We can select one of these channels (in this case, an infrared one) and plot it in a similar way to the previous example involving HRV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonhrv[\"data\"].sel(time=\"2020-07-20 10:00\", channel=\"IR_016\").plot()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the HRV data, you are given 128x128 crops for all 11 non-HRV channels centred on each solar PV site taken over the previous hour. Just note that the non-HRV data, having a lower resolution, corresponds to a much larger area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(12, len(nonhrv.channel), figsize=(16, 18))\n",
    "\n",
    "for ax, channel in zip(axes[0], nonhrv.channel):\n",
    "    ax.set_title(channel.item(), rotation=0, size=\"large\")\n",
    "\n",
    "for i, time in enumerate(\n",
    "    [\n",
    "        \"2020-07-04 12:00\",\n",
    "        \"2020-07-04 12:05\",\n",
    "        \"2020-07-04 12:10\",\n",
    "        \"2020-07-04 12:15\",\n",
    "        \"2020-07-04 12:20\",\n",
    "        \"2020-07-04 12:25\",\n",
    "        \"2020-07-04 12:30\",\n",
    "        \"2020-07-04 12:35\",\n",
    "        \"2020-07-04 12:40\",\n",
    "        \"2020-07-04 12:45\",\n",
    "        \"2020-07-04 12:50\",\n",
    "        \"2020-07-04 12:55\",\n",
    "    ]\n",
    "):\n",
    "    for j, channel in enumerate(nonhrv.channel):\n",
    "        axes[i][j].imshow(\n",
    "            nonhrv[\"data\"]\n",
    "            .sel(time=time, channel=channel)\n",
    "            .isel(x_geostationary=slice(128, 256), y_geostationary=slice(128, 256))\n",
    "            .to_numpy(),\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "        axes[i][j].get_xaxis().set_visible(False)\n",
    "        axes[i][j].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, whenever you read a slice of data from a Zarr data array, all the chunks corresponding to your selection need to be individually fetched and decompressed. As an example, the non-HRV data, which has dimensions `(time, y_geostationary, x_geostationary, channel)`, is stored as chunks with the shape `(1, 293, 333, 11)`, meaning that each chunk contains the data for all 11 non-HRV channels at a single time step. \n",
    "\n",
    "Especially when you account of the chunk caching that takes place under the hood, this means that it is significantly faster to select multiple non-HRV channels for the same time step together (since they are all stored within the same chunk) than it is to select multiple time steps' worth of data for each channel separately. You can see this effect by swapping the order of the loops in the code block above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Forecasts\n",
    "\n",
    "We can also look at the weather forecast dataset by loading and visualising it in a very similar way!\n",
    "\n",
    "As you can see, this dataset is composed of 38 different data variables (many of which correspond to different altitudes), such as for ground temperatures, total precipitation and more. Forecasts are available on an hourly basis. For further information on each of these data variables, check out the data section on the [ClimateHack.AI 2023 competition page](https://doxaai.com/competition/climatehackai-2023/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwp = xr.open_dataset(\n",
    "    \"zip:///::https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/weather/2020/7.zarr.zip\",\n",
    "    engine=\"zarr\",\n",
    "    consolidated=True,\n",
    ")\n",
    "\n",
    "nwp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground temperatures\n",
    "\n",
    "Just as with the satellite imagery data, we can also plot individual data variables in the weather forecast dataset. Here, `t_g` corresponds to ground-level temperatures in Kelvin (which we convert to Celsius in the visualisation below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "(nwp[\"t_g\"].sel(time=\"2020-07-20 10:00\") - 273.15).plot.pcolormesh(\n",
    "    ax=axes,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    add_colorbar=True,\n",
    "    cmap=\"coolwarm\",\n",
    ")  # type: ignore\n",
    "\n",
    "axes.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the satellite imagery, we can generate 128x128 crops for each weather forecast data variable as well. As part of this challenge, you are given six time steps' worth of weather forecast data, corresponding to the beginnings of the previous hour, the current hour and the next four hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 6, figsize=(15, 3))\n",
    "\n",
    "for j, time in enumerate(\n",
    "    [\n",
    "        \"2020-07-04 10:00\",\n",
    "        \"2020-07-04 11:00\",\n",
    "        \"2020-07-04 12:00\",\n",
    "        \"2020-07-04 13:00\",\n",
    "        \"2020-07-04 14:00\",\n",
    "        \"2020-07-04 15:00\",\n",
    "    ]\n",
    "):\n",
    "    ax[j].imshow(\n",
    "        nwp[\"t_g\"]\n",
    "        .sel(time=time)\n",
    "        .isel(latitude=slice(100, 228), longitude=slice(100, 228))\n",
    "        .to_numpy(),\n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "    ax[j].get_xaxis().set_visible(False)\n",
    "    ax[j].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud cover\n",
    "\n",
    "Similarly, we can also look at total cloud cover forecasts (`clct`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "(nwp[\"clct\"].sel(time=\"2020-07-20 10:00\") - 273.15).plot.pcolormesh(\n",
    "    ax=axes,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    add_colorbar=True,\n",
    ")  # type: ignore\n",
    "\n",
    "axes.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All weather variables\n",
    "\n",
    "Here are all the weather variables available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 8\n",
    "ncols = 5\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(10, 20),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    ")\n",
    "\n",
    "for j, var in enumerate(nwp.data_vars):\n",
    "    nwp[var].sel(time=\"2020-07-20 10:00\",).plot.pcolormesh(\n",
    "        ax=axes[j // ncols][j % ncols],\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        x=\"longitude\",\n",
    "        y=\"latitude\",\n",
    "        add_colorbar=False,\n",
    "        cmap=\"coolwarm\" if var.split(\"_\")[0] in (\"t\", \"v\", \"u\") else \"viridis\",\n",
    "    )\n",
    "\n",
    "    axes[j // ncols][j % ncols].coastlines()\n",
    "    axes[j // ncols][j % ncols].get_xaxis().set_visible(False)\n",
    "    axes[j // ncols][j % ncols].get_yaxis().set_visible(False)\n",
    "    axes[j // ncols][j % ncols].set_title(var)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the challenge, you get to be really creative with what data variables you choose to use and integrate into your model. Not all of them will necessarily be relevant to you, but here are some interesting ones to potentially look at first:\n",
    "\n",
    "- `alb_rad`: Surface albedo (%)\n",
    "- `clch`: Cloud cover at high levels (0-400 hPa) (%)\n",
    "- `clcl`: Cloud cover at low levels (800 hPa - soil) (%)\n",
    "- `clcm`: Cloud cover at mid levels (400-800 hPa) (%)\n",
    "- `clct`: Total cloud cover (%)\n",
    "- `t`: Air temperature (K) ‚Äì available for three different pressure levels (at 5600m, 1500m and 500m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air Quality Forecasts\n",
    "\n",
    "Finally, we can also explore the ECMWF CAMS air quality forecast dataset, which contains a number of data variables related to aerosols in the atmosphere at 8 different levels. There is a lot of aerosol data available, so if you are interested in using the aerosol data as part of your submission, it is worth spending some time to get familiar with the data and figure out which data variables may actually be useful to you. For example, not all aerosol types are found in large concentrations over Great Britain, which is our area of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerosols = xr.open_dataset(\n",
    "    \"zip:///::https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/aerosols/2020/7.zarr.zip\",\n",
    "    engine=\"zarr\",\n",
    "    consolidated=True,\n",
    ")\n",
    "\n",
    "aerosols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access a list of available data variables programmatically through the `data_vars` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerosols.data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also inspect the different levels (or altitudes) for which the aerosol forecasts are available for each data variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerosols.level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise the aerosol forecasts in a similar way to the satellite imagery and weather forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "aerosols[\"pm10_conc\"].sel(time=\"2020-07-20 10:00\", level=1000).plot.pcolormesh(\n",
    "    ax=axes,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    add_colorbar=True,\n",
    ")  # type: ignore\n",
    "\n",
    "axes.coastlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=len(aerosols.data_vars),\n",
    "    ncols=len(aerosols.level),\n",
    "    figsize=(15, 28),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    ")\n",
    "\n",
    "for j, var in enumerate(aerosols.data_vars):\n",
    "    for i, level in enumerate(aerosols.level):\n",
    "        aerosols[var].sel(time=\"2020-07-20 10:00\", level=level).plot.pcolormesh(\n",
    "            ax=axes[j][i],\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            x=\"longitude\",\n",
    "            y=\"latitude\",\n",
    "            add_colorbar=False,\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "\n",
    "        axes[j][i].coastlines()\n",
    "        axes[j][i].get_xaxis().set_visible(False)\n",
    "        axes[j][i].get_yaxis().set_visible(False)\n",
    "        axes[j][i].set_title(f\"{var} ({int(level)}m)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate 128x128 crops of the air quality forecast data in a similar way to the weather data, with the main difference being that the air quality forecasts have an additional `level` dimension. Otherwise, for this challenge, you are given data for the same six time steps as the weather forecast data, namely forecasts associated with the beginnings of the previous hour, the current hour and the next four hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, len(aerosols.level), figsize=(16, 12))\n",
    "\n",
    "for ax, channel in zip(axes[0], aerosols.level):\n",
    "    ax.set_title(channel.item(), rotation=0, size=\"large\")\n",
    "\n",
    "for i, time in enumerate(\n",
    "    [\n",
    "        \"2020-07-04 10:00\",\n",
    "        \"2020-07-04 11:00\",\n",
    "        \"2020-07-04 12:00\",\n",
    "        \"2020-07-04 13:00\",\n",
    "        \"2020-07-04 14:00\",\n",
    "        \"2020-07-04 15:00\",\n",
    "    ]\n",
    "):\n",
    "    for j, level in enumerate(aerosols.level):\n",
    "        axes[i][j].imshow(\n",
    "            aerosols[\"pm2p5_conc\"]\n",
    "            .sel(time=time, level=level)\n",
    "            .isel(latitude=slice(100, 228), longitude=slice(100, 228))\n",
    "            .to_numpy(),\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "        axes[i][j].get_xaxis().set_visible(False)\n",
    "        axes[i][j].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that you have had an overview of the data available, you are now ready to train your first model and make your first submission for ClimateHack.AI 2023! ü•≥\n",
    "\n",
    "See the Jupyter notebook `2_training.ipynb` for the next part of these getting started materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
